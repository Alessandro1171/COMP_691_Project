# Generated 2025-04-18 from:
# /content/hparams_ECAPA-TDNN.yaml
# yamllint disable
seed: 1986
__set_seed: !apply:speechbrain.utils.seed_everything [1986]

# Set up folders for reading from and writing to
# Dataset will be downloaded to the `data_folder`
data_folder: ./Italian_Parkinsons_Voice_and_Speech/italian_parkinson      # e.g. /localscratch/common_voice_kpd/
output_folder: results/ECAPA-TDNN/1986
save_folder: results/ECAPA-TDNN/1986/save
train_log: results/ECAPA-TDNN/1986/train_log.txt
train_csv: results/ECAPA-TDNN/1986/save/train.csv
dev_csv: results/ECAPA-TDNN/1986/save/dev.csv
test_csv: results/ECAPA-TDNN/1986/save/test.csv
skip_prep: false

# Data for augmentation
NOISE_DATASET_URL: 
  https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1
RIR_DATASET_URL: 
  https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1
data_folder_noise: ./Italian_Parkinsons_Voice_and_Speech/italian_parkinson/noise # The noisy sequences for data augmentation will automatically be downloaded here.
data_folder_rir: ./Italian_Parkinsons_Voice_and_Speech/italian_parkinson/rir # The impulse responses used for data augmentation will automatically be downloaded here.
noise_annotation: results/ECAPA-TDNN/1986/save/noise.csv
rir_annotation: results/ECAPA-TDNN/1986/save/rir.csv

# The train logger writes training statistics to a file, as well as stdout.
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: results/ECAPA-TDNN/1986/train_log.txt

error_stats: !name:speechbrain.utils.metric_stats.MetricStats
  metric: !name:speechbrain.nnet.losses.classification_error
    reduction: batch

####################### Training Parameters ####################################

# Feature parameters btw: 40 - 80
n_mels: 80
sample_rate: 16000
number_of_epochs: 30
batch_size: 2
n_statuses: 2
emb_dim: 192 # dimensionality of the embeddings
emb_channels: &id001 [64, 64, 64, 64, 128]
emb_attention_channels: 128

# Dataloaders
num_workers: 1
drop_last: true
train_dataloader_options:
  num_workers: 1
  batch_size: 2
  drop_last: true
  shuffle: true

test_dataloader_options:
  num_workers: 1
  batch_size: 2
  shuffle: true

############################## Augmentations ###################################

# Download and prepare the dataset of noisy sequences for augmentation
prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL
  URL: 
    https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1
  dest_folder: ./Italian_Parkinsons_Voice_and_Speech/italian_parkinson/noise
  ext: wav
  csv_file: results/ECAPA-TDNN/1986/save/noise.csv

# Download and prepare the dataset of room impulse responses for augmentation
prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL
  URL: 
    https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1
  dest_folder: ./Italian_Parkinsons_Voice_and_Speech/italian_parkinson/rir
  ext: wav
  csv_file: results/ECAPA-TDNN/1986/save/rir.csv

# Add reverberation to input signal
add_reverb: !new:speechbrain.augment.time_domain.AddReverb
  csv_file: results/ECAPA-TDNN/1986/save/rir.csv
  reverb_sample_rate: 16000
  clean_sample_rate: 16000
  num_workers: 1

# Add noise to input signal
add_noise: !new:speechbrain.augment.time_domain.AddNoise
  csv_file: results/ECAPA-TDNN/1986/save/noise.csv
  snr_low: 0
  snr_high: 15
  noise_sample_rate: 16000
  clean_sample_rate: 16000
  num_workers: 1

# Speed perturbation
speed_perturb: !new:speechbrain.augment.time_domain.SpeedPerturb
  orig_freq: 16000

# Augmenter: Combines previously defined augmentations to perform data augmentation
wav_augment: !new:speechbrain.augment.augmenter.Augmenter
  concat_original: false
  shuffle_augmentations: false
  min_augmentations: 1
  max_augmentations: 3
    #augmentations: [
    #    !ref <add_reverb>,
    #    !ref <add_noise>,
    #    !ref <speed_perturb>]

# Feature extraction
compute_features: &id002 !new:speechbrain.lobes.features.Fbank
  n_mels: 80

# Mean and std normalization of the input features
mean_var_norm_input: &id004 !new:speechbrain.processing.features.InputNormalization
  norm_type: sentence
  std_norm: false

############################## Models ##########################################

# To design a custom model, either just edit the simple CustomModel
# class that's listed here, or replace this `!new` call with a line
# pointing to a different file you've defined.

# Embedding Model
embedding_model: &id003 !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN
  input_size: 80
  activation: !name:torch.nn.LeakyReLU
  channels: *id001
  kernel_sizes: [5, 3, 3, 3, 1]
  dilations: [1, 2, 3, 4, 1]
  attention_channels: 128
  lin_neurons: 192

# Classifier based on cosine distance
classifier: &id005 !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier

# Additive Angular Margin
  input_size: 192
  out_neurons: 2

# The first object passed to the Brain class is this "Epoch Counter"
# which is saved by the Checkpointer so that training can be resumed
# if it gets interrupted at any point.
epoch_counter: &id006 !new:speechbrain.utils.epoch_loop.EpochCounter

# Load pretrained embedding module
# Note: in this case, we pre-train with the ECAPA-TDNN model trained on voxceleb
# for speaker-id (this leads to a performance improvement).
#embedding_model_path: speechbrain/spkrec-ecapa-voxceleb/embedding_model.ckpt

# Pretrained ECAPA embeddings from SpeakerID on VoxCeleb
#pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
#    collect_in: !ref <save_folder>
#    loadables:
#        embedding_model: !ref <embedding_model>
#    paths:
#        embedding_model: !ref <embedding_model_path>
  limit: 30

# Objects in "modules" dict will have their parameters moved to the correct
# device, as well as having train()/eval() called on them by the Brain class.
modules:
  compute_features: *id002
  embedding_model: *id003
  mean_var_norm_input: *id004
  classifier: *id005
compute_cost: !new:speechbrain.nnet.losses.LogSoftmaxWrapper
  loss_fn: !new:speechbrain.nnet.losses.AdditiveAngularMargin
    margin: 0.2
    scale: 30

# Learning rates
lr: 0.0002
lr_final: 0.00001


# This optimizer will be constructed by the Brain class after all parameters
# are moved to the correct device. Then it will be added to the checkpointer.
opt_class: !name:torch.optim.Adam
  lr: 0.0002
  weight_decay: 0.000002


# Linear lr decay
lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler
  initial_value: 0.0002
  final_value: 0.00001
  epoch_count: 30

############################## Logging and Pretrainer ##########################

# This object is used for saving the state of training both so that it
# can be resumed if it gets interrupted, and also so that the best checkpoint
# can be later loaded for evaluation or inference.
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: results/ECAPA-TDNN/1986/save
  recoverables:
    normalizer_input: *id004
    embedding_model: *id003
    classifier: *id005
    counter: *id006
